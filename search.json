[{"title":"xv6-address-space","date":"2022-05-02T09:28:13.000Z","url":"/2022/05/02/xv6-address-space/","tags":["os","xv6"],"content":"第三章、页表分页机制xv6运行在Sv39的RISC的机器上，39表示用到了机器64位的低39位，高25位没有用到。在这样的设置中，逻辑页表是一个2的27次方大小的页表项数组（39 - 12 &#x3D; 27）。每个页表项包括一个44位的物理页号，以及10位的校验位。实际使用用不了这么多的物理页，因为2的54次方太大的，因此物理页号的高位是0。RISC-V的实现是三级页表，一个虚拟地址经过页表项的三次翻译变成物理地址。页表保存在内存中，构成一个三层的树。树根是4k大小的一页，包括512（4k &#x2F; 512 &#x3D; 8byte &gt; 54bit）个页表项PTE。页表就是对每个进程地址空间的描述。 为了打开页表机制，需要把根页表的地址写入satp寄存器。每个CPU有一个单独的satp寄存器。因此每个CPU可以根据不同的页表进行地址转化。在进入内核态时，satp寄存器指向内核的根页表地址。在用户进程中，寄存器指向用户进程的根页表地址。在系统最初初始化的时候，在kernel/start.c中就写apt寄存器为0，即禁用分页机制。 Direct Mapping内核为了能够处理一些硬件资源等，会把一部分的物理地址直接映射到同一个虚拟地址，这个叫做“direct mapping”。例如，在物理地址以及虚拟地址，内核的起始地址都位于0x80000000。直接内存映射简化了内核读写物理内存。例如，fork创建子进程，内存分配器返回物理内存，然后内核把这个地址直接当成虚拟地址进行处理，省略了一步地址转化。 同时，还有一部分内存不是直接映射的： trampoline的内存页：这个内存页被映射到了虚拟内存的地址最高端（trampoline在内核中被映射了两次，一次到地址最高端，一次是直接映射） 内核栈页面：每个进程都有一个内核栈，这个内核栈映射到地址高端，防止可能的地址越界造成对其他正在使用页面的污染。（把这个很重要的数据保护起来，放在平常访问不到的地址高端） 使用直接映射的内核栈地址也可以，就是这样做不好处理可能的内存越界，因为和其他的数据挨得比较近。如果有地址越界，造成其他页面的数据被覆盖，单纯的内核异常不能处理这个问题。 每个CPU有一个寄存器SATP寄存器，这个寄存器存放的是当前进程的第一级页表的地址，这个地址，以及页表项PTE里面的地址（索引下一级的页表），这个地址都是物理地址。因为页表的目的就是虚拟地址与物理地址的转换，这儿还是虚拟地址的话，就陷入了无尽的循环。 因为三级页表的存在，翻译一个虚拟地址需要读内存三次才能转为物理地址。这个太慢了，于是有了TLB的存在，这个是一个硬件CPU层面东西，我们只需要知道他的存在就行。在RISCV中，指令sfence_vma 清空TLB缓存。不同的进程有自己不同的页表，在进程切换时，TLB也需要清空。 xv6中的walk函数实现了MMU硬件相同的功能 既然有MMU的硬件，那为啥需要一个walk函数，实现相同的逻辑？ 解释一下，这个就是说，通常情况下，每个进程只需要用到自己的内存空间，使用自己的pagetable。这样的话，pagetable由MMU实现，不需要walk函数是完全可以的。但是针对copyin、copyout这种情况，内核进程需要直接对用户进程的内存空间进行数据的读写。就是需要访问其他的进程，这个需要显式的调用。也就是为啥会有walk函数的存在。不知道说的对不对？ 用户进程空间布局 其中，program break就指的是内存布局中data段的末尾。brk、sbrk的语义就是把program break后移，空出新的内存空间，返回给用户。这个就是heap堆区的增长方式。 sbrk分配器 xv6中的这个slab和linux中的slab不一样，linux中的slab是针对字节大小的小内存分配的设计。 用户进程的malloc函数会调用sbrk这个系统调用user/umalloc.c,然后调用kernel/proc.c中的growproc,即扩张program brea;接着调用uvmalloc,可以看到这种分配方式是以页面为单位的，计算了oldsz、newsz之间差几个页面，然后kalloc分配对应的物理页面，并且映射到相应的program break位置上去。最后更新p-&gt;sz &#x3D; newsz; buddy分配器 参考ULK,312页伙伴系统 参考MOSPI,62页buddy system, slab buddy分配器把所有空闲页分为多个链表，每个块链表包含大小为1，2，4，8…的物理连续页框。 步骤： 需要大小为n个页面的内存，先对n上取整到2^k; 在buddy的链表上对应的2^k上找，找到返回，并更新bitmap 没找到，在更大的块链表上找，找到一半返回给用户，一半加入上一级链表；以此类推。 维护bitmap，合并buddy。 怎么确定buddy中的伙伴？ buddy数组中的每一项指向一个链表，分表表示大小为1，2，4，8…个连续物理页面的地址 以第一个链表，即大小为一个页面的为例。假设第一个物理页面地址0x0000,则第二个物理页面地址0x1000,两者互为伙伴。地址只差一位，而且在第12位，而且这个链表大小4K(2^12)。 malloc与free的实现 TODO 参考TLPI，page117 Lab: Allocator for xv6任务 修改kernel/file.c，使用buddy分配器，使得打开文件数不受NFILE限制 buddy分配器空间利用不高效。对于buddy系统关系的每种大小的每个内存块，需要用一位来记录是否在使用。一个优化是对于buddy中的一对内存块，只是用一位表示，即B1_is_free异或B2_is_free。对于一对buddy内存B1、B2,每当有一个块被释放或者分配，就翻转这个数。例如：如果B1、B2都分配了则置为为0，如果B1被释放，翻转为1。如果这一位是1并且B2被释放，我们就需要合并B1B2。这样buddy管理的每个块节省了1&#x2F;2个bit。当buddy管理128M内存的时候，大概可以节省1MB内存。 整个lab只需修改kernel/buddy.c和kernel/file.c Hints 移除kernel/file.c的第19行。在filealloc中使用bd_malloc分配struct file结构体。然后在fileclose中释放分配的内存 fileclos需要获得ftable.lock，因为这个锁保护f-ref bc_alloc并不会清空分配得到的内存，需要在使用前由调用者手动清空 使用bd_print打印分配器的状态 与讲义相比，我们修改了bd_init。因此，这个函数调用的内存是他可用的物理内存范围，与讲义相比，我们已经修改了bd_init，因此可以将其调用，可用于分配的物理内存范围。 BD_Init从该内存分配了Buddy数据结构的内存。它相应地初始化了数据结构：BD_INIT标记了用于分配的好友数据结构的内存，以免重新分配。此外，我们已经修改了BD_INIT，以处理一定数量的内存，这些内存不是通过将不可用的内存标记为分配的2的功率。最后，我们修改了伙伴分配器，以使用锁来序列化并发调用。 选做 把其他的数据结构改为动态申请内存的方式，proc数据结构需要大幅修改，别的还可。 TODO: 先把buddy.c看明白orz xv6中的buddy和linux中的还有不同，linux中的buddy是针对物理页面的一种内存管理器。在xv6中buddy管理的内存粒度似乎要更小？最小的内存块大小LEAF_SIZE &#x3D; 16字节？？在系统初始化之后所有的内存全部交给buddy管理，内存大小几乎等于128MB。所以buddy数组的大小也大得多，长度为24。 互为buddy的块物理内存连续，在xv6的buddy中，采用序号的方式.互为buddy的两个块的序号为(n,n+1)其中n为偶数。 xv6-buddy源码bd_init首先调试一下main.c，其中用到了buddy 来初始化内核物理内存。过程中发现一个错误，riscv64-unknown-elf-gdb调试器无法打印出local variable，最后解决参考，在CFLAGS中加一个参数-gdwarf-2。 首先进入到bd_init;bd_init初始化了buddy的元数据部分，即地址0x8002000到0x88000000;然后调用bd_initfree初始化空闲内存。 分别初始化这个长度为24的数组的alloc以及split。他的长度表示这个大小的块的最大个数，每个块的状态用一位表示。 因为块大小为16B的已经是最小的，必能继续分割了。所以其没有split数组。代码中从下表1开始 然后把buddy系统的元数据所占用的内存在buddy系统中标记为1，即在使用；同样的把end地址上取整的部分地址是不可用的，同样标记为1。 在kinit之后，打印出buddy的状态 根据代码注释，这儿的alloc作为bitmap记录那些亏被分配出去了，split记录那些块被分割了；但是为啥打印处理的alloc和split一样呢？TODO 根据这个输出布局可以看到，alloc已经分配出去的内存分为两部分，一部分是前面的buddy的元数据，后面的一块内存是上取整留出来的无法使用的内存。然后观察对于每种块大小的freelsit、组合起来刚好完全覆盖了空闲内存区域 以大小为16B的做个解释：每块大小16B,最多有8388604个块，共计128MB内存。freelist只有一个，说明只有管理内存的左侧或者右侧有一个空间，其余空闲都在严格内部。196657*16&gt;&#x3D;3146512,即表示0到196657这些块被buddy的元数据占用；8388608-8377856&#x3D;0x2a00，表示从8388608到8377856的物理page属于不可用的页面。 有的没有freelist，说明管理的可用内存刚好margin&#x3D;块大小，buddy彼此被完整分割 有的freelist不止一个，说明管理的可用内存左右边界刚好各自用了一个块，剩下一个buddy成为free空闲状态。 内存布局 空闲内存初始化；通过这种方式，其实就已经把所有的空闲内存区域管理起来了；块初始化示意图 这块还不是很懂，画的图可能有bug ![Untitled Diagram.drawio]( Diagram.drawio.png) bd_malloc bd_free lab怎么做 orz "},{"title":"xv6-trap","date":"2022-04-26T13:53:04.000Z","url":"/2022/04/26/xv6-trap/","tags":["os","xv6","riscv"],"content":"xv6的trap机制syscall相关的riscv寄存器： stvec：ecall 指令跳转到这儿执行，这个寄存器存放的是trampoline的地址；trampline汇编代码是用户空间进如内核空间的代码 sepc：ecall指令把用户空间的pc指针保存在此 scause：ecall将其设置为8，表示一个系统调用 sscratch： trapframe的地址 sapt（S-mode address translation and protection）：寄存器保存当前的页表基址 a0-a7：系统调用的参数 ra：返回地址 a0：返回值 a7：syscall的号码 tp：当前cpu线程号 Sv39地址翻译可以看到，虚拟地址用到了64位中的低39位，即最大的虚拟地址空间0x4000000000 以write为例，说明系统调用的过程： write() trampoline &#x2F; uservec usertrap() syscall() sys_write() usertrapret() trampoline &#x2F; userret write()返回 以sh.c中的write系统调用为例子说明： 首先查看sh.c的objdump代码sh.asm，查看到write函数直接调用的是user/usys.S的write函数，并且地址是0000000000000d6a，然后进入gdb调试。 在执行ecall指令之前，查看各个寄存器的值，包括pc、sapt、sepc、stvec、scause、sscratch以及a0 然后si单步执行ecall，进入到kernel/trampoline.S执行， 首先，把sscratch寄存器与a0寄存器的值交换，然后此时a0寄存器就是trapframe的地址，即要把当前进程的寄存器保存到a0寄存器所存的地址对应的页表中。 最后跳转到进程结构体p-&gt;tf-&gt;kernel_trap处执行，（这个在kernel/trap.c中usertrapret中设置进程的内核trap处理函数为usertrap） 然后进入kernel/trap.c的usertrap执，因为已经进入内核，首先在函数中切换中断处理为kernelvec 判断sscause寄存器的值是否为8，如果是8，就说明是来自用户空间的syscall。然后把寄存器epc的值保存到当前进程结构体中，并且epc+4，即反悔的时候直接执行ecall的下一条指令。 最后调用syscall()，根据寄存器a7选择相应的系统调用处理函数，最后把返回值置给寄存器a0，最后返回。 首先回到usertrapret函数中，设置进程关于内核的一些参数，如内核satp、kernel_sp进程的内核栈地址，进程trap的处理函数等，最后跳转到trampoline的userret处执行，重新从trapframe中把各个寄存器数据回复到物理的寄存器上，然后返回用户进程。 以initcode的系统调用为例： 执行ecall： 进入usertrap 设置stvec寄存器为kernelvec，表示已经进入内核，相应的中断处理要由内核完成。 判断scause寄存器的值，如果是8，表示syscall，先把epc+4，即返回的时候直接执行ecall的下一条指令 执行syscall函数，根据a7寄存器执行sys_xxx，返回值放在a0寄存器 进入usertrapret 关中断，写stvec寄存器为trampoline的userret。当当前不处理其他的中断，唯一处理的是进程从内核返回用户态，使用的代码是trampoline中的userret。 设置好各种参数，包括设置epc为进程保存的epc、然后把sapt寄存器设置为当前进程的页表基址 最后执行trampoline中的userret，回复进程现场，返回到用户态，继续执行ecall的下一条指令 参考 gdb调试：gdb调试有一个.gdbinit ，可以吧进入gdb之后的固定配置写到里面。执行命令gdb。首先执行这些命令。 进程优先级：通过nice 调整，nice的取值-20-19，数字越大，优先级越低 taskset：绑定进程到某个CPU执行 "},{"title":"xv6-lab-shell","date":"2022-04-23T07:03:17.000Z","url":"/2022/04/23/xv6-lab-shell/","tags":["os","xv6","riscv"],"content":"Lab: Simple xv6 shell写一个xv6的shell，可以处理输入输出、重定向、管道。 shell文件应该位于user/nsh.c，然后更新makefile，自己实现的shell中的prompt应该使用符号@，以便于和原本的shell的$区分。 hints 不要使用动态内存分配如malloc，只能使用局部变量以及全局变量。（但是这个感觉有点骗测试的样子orz） 可以做出一些强制限制，比如命令的长度、参数的个数、每个参数的长度等（便于静态声明变量） nsh的测试文件为testsh，运行方式为testsh nsh 不用merge之前的实验，也基本用不到之前的代码。 不用实现testsh中没有的测试点，比如对括号以及引用的解析 参考K&amp;R的C书， 比如5.12节的gettoken很有用 xv6的shell复杂的多，可以借鉴。复用代码须注释说明 可以使用用户空间的库user/ulib.c 及时关闭不使用的文件描述符 所有的系统调用需要检查返回值，是否正确 testsh会重定向shell的标准输出，在shell中把error以及debug信息输出到fd 2。 关注testsh中的one函数，可能会有用。 intend： 这个lab主要应该还是让用一用syscall，理解syscall过程。然后多用一用fork、exec、pipe，redirect等等吧。 那些东西是父子进程共享的，那些是独有的。然后看代码。 管道：|重定向：&gt;、&lt;对于管道和重定向同时存在的情况，管道的优先级要高，那就是先处理管道、再处理重定向。 管道把命令分成了两部分，先执行管道左侧的命令，然后创建管道，写入数据；再执行右侧命令，从管道读数据，处理。 重定向负责把标准输入、输出、错误重定向到某个文件？ 对于重定向输出，fork执行&gt;左侧的命令的时候，设置重定向 对于重定向输入，fork执行&lt;右侧的命令的时候，设置重定向 示例 Bugs 遇到的第一个问题就是不太会处理字符串，然后也没有用sh.c中的其他函数。 第二个问题是不会读数据，最后发现构造的测试样例是每个命令以\\n结尾。然后就是在读输入时，遇到\\n就break，如果读到文件结束就退出，测试层序会多次调用nsh进程。 第三个问题就是我知道posix管道是半双工的，但是在父进程创建管道之后，写管道子进程没有关闭管道的读管道描述符；读管道子进程也没有关闭写管道描述符，然后造成了程序的undefined behavior，太蠢了。最后在ULK的page768找到解决方案。 最后一个坑是先写管道，写完之后要及时关闭写管道描述符，我只是在子进程中关闭了写管道描述符，但是在父进程中没有关（管道可以被多个进程读写、需要加锁保证数据可用），导致了后半段命令的子进程一直sleep，不能读到管道EOF，然后退出进程。 这个问题在源代码中反映为user&#x2F;nsh.c中88-91行，debug之前是先两个wait，然后两个close。太蠢了。。。 xv6-book片段 fork和exec分成两步的原因就是在这两个徐彤调用中间可以插入一个IO重定向的部分。这样就能在不修改代码的情况下实现多个程序的连接 fork以及dup系统调用之后，父子进程对于同一个打开的文件描述符共享同一个文件偏移，因此下面的代码会输出hello,world到文件。这也是多个shell命令实现的方式。 2&gt;&amp;1：这个命令的意思是对于shell命令，把文件描述符2设置为文件描述符1的复制，即 这样做，可以让shell把标准输出和标准错误合并到同一个输出文件中。 管道pipe中，写管道结束之后，应该关闭管道的写一侧，否则管道的读一侧会一直阻塞。 "},{"title":"xv6-intro","date":"2022-04-20T14:11:55.000Z","url":"/2022/04/20/xv6-intro/","content":"配置 环境：ubuntu20.04 跟着xv6-riscv-fall19做的： Lab: Xv6 and Unix utilities： 安装依赖： clone代码： 测试环境： 启动首先在kernel.ld这个链接脚本中设置了起始的地址为0x80000000，这个地址也是qemu中首先跳转执行的地址。对应到源码中是kernel/entry.S，可以看到entry.S代码给每个硬件线程设置好sp寄存器，然后跳转到start。 其中la指令是伪指令（load address），stack0在start.c中定义，为每个cpu准备一个栈，每个栈大小写096 li（load immediate），设置寄存器a0为4096 csrr(Control System Register)控制系统寄存器，读取数据到寄存器a1 上面的步骤设置为start函数设置了相应的sp寄存器。然后进入kernel/start.c的start()函数中。 在start函数中，进行各种设置，设置异常程序计数器指针为main函数，即异常处理程序。在start函数中，设置计时器开始， Util 实现用户进程sleep 先看xv-book的第一章 参考其他的例子：user/echo.c，user/grep.c 等怎么从命令行接受参数 如果用户使用sleep命令没有给出参数，打印出错误信息 命令行参数是string类型，使用user/ulib.c中的函数atoi转换 使用sleep系统调用 在kernel/sysproc.c中查看sleep系统调用的实现，在user/.h中查看用户空间的sleep调用，在user/usys.S中查看从用户程序跳入内核的汇编程序。 最后确保sleep函数调用了exit()来退出程序。 在Makefile中的UPROGS中添加自己是先的sleep函数，然后编译，就可以在shell运行sleep。 额外的函数：实现uptime用户程序，打印系统启动uptime的ticks数。 解析： 在kernel/start.c中的start函数中，调用了timerinit()，其中注释提到时钟中断间隔为1000000个interval、大概是1&#x2F;10秒。暂停用户定义的ticks，即sleep参数和用户输入一致。 实现用户进程pingpong 利用管道实现一个ping-pong程序，父进程写入parent_id[1]一个字节，子进程从parent_id[0]读取这个字节，子进程读完之后，把数据写入child_id[1]，然后父进程再从child_id[0]读出来。 使用pipe创建管道 使用fork创建子进程 使用read、write读写数据 使用getpid得到进程号 这个比较简单，还用不到覆盖标准输入输出等；重定向 解析：pipe系统调用怎么实现的 实现用户进程primes 关于管道、重定向 写一个并发版本的素数过滤器， 使用fork以及pipe建立流水线、第一个进程把数字2-35打入pipe，从左到右输出每一个在其中的素数。 参考这个：，这个图的说明已经非常清楚了。 实现用户进程find 先看user/ls.c，明确怎么读目录 递归的遍历子目录 不用遍历.和.. 对文件系统的更改是持久化的，需要make clean、然后重新make qemu 要用到C语言的strings 实现用户进程xargs 从标准输入读入多行，然后依次执行 对于每行输入，使用fork以及exec系统调用。在父进程中使用wait等待子进程结束 每次从标准输入读一个字符，知道读到换行符\\n 参考kernel&#x2F;param.h，中的MAXARG 对文件系统的更新是持久化的，通过make clean，重新编译make qemu 这儿遇到一个问题，在有argv的情况下，怎么读stdin？ 最后发现是我太蠢了，运行程序./xargs arg1 arg2 ...[\\n回车]，程序运行了，然后接下来的输入才对应的是从标准输入中独到的内容。 最后遇到一个问题，给argv直接追加项，导致没追加签的argv的最后一项的值变了，最后把srgv以及标准输入读的参数复制到另一个数组解决。 文件系统相关概念在find程序中，参考了ls。find的格式是find &lt;path&gt; &lt;file&gt;，即在路径下找到匹配的文件。那么首先即使要打开path这个文件，所有的路径对应于一个文件。 在文件系统的实现中，文件和目录其实没啥区别，区别仅在于目录文件中放的是目录信息。而文件里面放的是自定义的数据。 因此在实现中，先判断path的类型。 如果是文件，判断是否和文件名相同，相同则输出。 如果是目录，就从次文件中读一个目录项，直到读完。针对每个读出来的目录项，递归的判断是文件还是目录，然后做相同的处理。 在ls程序中，打印出了路径下的所有“文件”，包括文件，目录， 设备（在kernel&#x2F;stat.h，1表示目录，2表示文件，3表示设备）。“文件”的stat结构体相当于“文件”的元数据，定义在kernel&#x2F;stat.h 其中dev表示的是disk device，表示哪一个磁盘，ino表示在这个磁盘上的inode号码，type表示哪种“文件”，有文件、目录、设备三种；nlink表示链接数量；size表示文件的大小。 通过在xv6中运行ls程序： 得到的结果表示所有的文件都在同一个磁盘设备，第二列表示文件类型，第三列表示inode号，根据第四列文件大小可以计算inode号。可以看到在这个文件系统中，一个inode负责1kb大小的磁盘空间。 TODO：关于生成文件系统这部分的代码在mkfs，随后在看。 关于怎么把kernel和fs.img链接起来，应该在kernel.ld 链接文件中或者makefile中，这个随后看。 Syscall在xv6的系统调用中，调用顺序是这样的： 首先在user目录下的是用户程序，如ls、cat等。 上面这些代码可能会使用到系统调用，用户的库的头文件是user/user.h，声明了系统调用的接口。 库文件的实现在user/usys.S汇编代码真正trap进入内核 riscv的系统功能的汇编指令：ecall，携带参数进入内核。其中系统调用号放在寄存器a7 由内核中的kernel/syscall.c中的syscall函数作为系统调用的相应函数，根据不同的中断号执行不同的系统调用sys_xxx函数。 C语言内嵌汇编参考： asm：表示后面的是汇编代码 volatile：表示编译器不需要优化代码，后面的指令保持原样 内嵌汇编语法：__asm__(汇编模板语句：输出部分：输入部分：破坏描述部分) %0,%1….代表参数，位置互相对应。 源码安装gdb8.3.1 参考资料 3.9 XV6 启动过程  (kernel). CSR（Control and Status Register Instructions）：读写寄存器 AUIPC（Add Upper Immediate to Program Counter）：这个指令把PC寄存器的高20位设置为操作数的和，低12位为0 在kernel/syscall.c中syscalls，系统调用数组声明为啥是这样的？；这样声明可以乱序初始化。emm "},{"title":"xv6-x86-boot","date":"2022-04-06T12:02:20.000Z","url":"/2022/04/06/xv6-x86-boot/","tags":["os","xv6"],"content":"[xv6 lab1] – Booting a PC计算机物理内存空间早期的8086等处理器的物理内存布局如图所示，是20位总线的机器，因此其最大寻址空间是1MB；物理内存低地址端的640KB位系统可用内存、640KB ~ 1MB位系统其他资源的占用内存，其中最重要的部分是从960KB ~ 1MB地址的BIOS部分；其他部分由RAM组成，BIOS部分由ROM制度存储实现，后来由可重复读写的flash实现，满足BIOS升级等功能。 从80286、80386开始机器总线扩展到32位，因此其最大物理寻址范围扩大到了4GB，为了物理内存布局的向后兼容性，在低地址端的1MB仍然保持了之前的内存布局。并且在32位机器上BIOS会使用4GB内存的高地址部分区域当作PCI设备的内存地址映射。因此在32位机器上，BIOS在整块内存上占了两个“洞”； Step1：使用qemu1、将实验环境搭建好，首先make 编译；可以看到编译之后新增加的文件，以及编译得到的结果obj/kern/kernel.img； 2、make qemu-gdb 命令使用qemu调试编译产生的镜像文件； 3、打开新的终端运行make gdb启动gdb调试； Step2：BIOS1、使用make gdb命令进入gdb调试，可以看到gdb执行的第一条汇编指令[f000:fff0] 0xffff0: ljmp $0x3630,$0xf000e05b；即第一条指令地址位于0xffff0；此处好像是qemu的原因，实际长跳转到地址0xfe05b处执行； 2、由第一小节的物理内存布局可以看到，地址0xffff0已经是BIOS区域的最后16字节了，16字节不足以完成BIOS的功能， 因此使用长跳转到BIOS内存低地址区域继续执行； 3、即BIOS完成了硬件自检、设备初始化等设置，然后从磁盘中读取0号扇区（引导扇区）到内存地址0x7c00处，每个扇区大小512B，因此引导扇区所占内存0x7c00 ~ 0x7dff，最后BIOS逻辑跳转到引导扇区[CS：0x0000, IP：0x7c00]执行、将控制权转移到内核引导器； 在20位机器或者在实模式中，物理地址physics_address = CS * 16 + IP； CS、IP寄存器都是16位寄存器；16位寄存器怎么表示20位地址偏移呢？使用CS向左偏移4位加上IP的值就可以组成了20位地址； BIOS干了什么：the BIOS intializes devices, sets of the interrupt routines, and reads the first sector of the boot device(e.g., hard-drive) into memory and jumps to it. Step3：Boot Loader在上一小节最后，BIOS通过跳转到0x7c00 将控制权交给内核引导器Boot Loader；Boot Loader在源码中主要由两个文件/boot/boot.S ，boot/main.c组成；其中Boot Loader做了两个重要操作：1、从实模式转到32位保护模式；2、加载操作系统内核到内存； 1、boot.s在简单初始化之后打开了地址总线的A20地址线 在早期的机器中总线是20位的，总线只有0~19，共1MB内存；后来内存扩展，为了保持向后兼容，A20地址线关闭表示地址译码为20位总线，打开表示使用更大的内存空间； 2、boot.s 将CR0（control register控制寄存器）寄存器的0位置为1，开启保护模式、从实模式切换到保护模式； CR0 寄存器； PE（Protection Enable）开启保护模式：切换到保护模式后寻址方式发生变化，不再是之前的CS + IP，而是通过段选择子GDT、IDT等方式内存寻址； PG （Paging） 开启分页机制 3、boot.s设置好栈指针寄存器的值，最后跳转到main.c中的bootmain函数执行； 内存布局中，栈指针只要给一个初值就行，此处设置的栈指针初值是start段的地址，就是说让栈指针指向start段的末尾，栈从该处开始增长； 4、上一步跳转到了bootmain 函数中，其中主要的代码片段如下； 总的来说，bootmain函数先把内核的ElF文件头4KB读入内存，根据ELF头中的信息继续把内核中的多个Program Header 读入内存，最后跳转到内核entey执行，把控制权交给内核； Layout of Kernel Program Header 通过objdump查看kernel的ELF文件的Program Header，标记为LOAD的就是要加载到内存中的； off：该Program Header 在此ELF文件中的偏移量； vaddr：该Program Header加载到内存中后在虚拟内存中的偏移量； paddr：该Program Header加载到内存后在物理内存中的偏移量； vaddr和paddr可能是由linker加载器手工设定的； 至此，内核完全加载完成； Memory Layout"}]